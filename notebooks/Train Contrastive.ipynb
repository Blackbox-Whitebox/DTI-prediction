{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fe86285",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc80425c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.nn.functional import cosine_similarity, normalize\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from trainer import Trainer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5478f2",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8907967f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, pred, labels):\n",
    "        (first, second) = pred\n",
    "\n",
    "        first = normalize(first, p=2, dim=1) # l2 norm\n",
    "        second = normalize(second, p=2, dim=1) # l2 norm \n",
    "\n",
    "        # (N, e) @ (e, N) -> (N, N)\n",
    "        # \n",
    "        \n",
    "        distance = torch.abs(cosine_similarity(first, second)) # (N)\n",
    "\n",
    "        loss = .5 * (labels * (1-distance) + (1-labels) * distance)\n",
    "        loss = loss.mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9d6a11",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0bebd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=1, dropout=0, bidirectional=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.gru = nn.GRU(embed_size, hidden_size, num_layers=num_layers, dropout=dropout, batch_first=True, bidirectional=bidirectional)\n",
    "        self.out = nn.Linear(hidden_size*2 if self.bidirectional else hidden_size, vocab_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, hidden=None):\n",
    "        x = self.embedding(x)\n",
    "        x, hidden = self.gru(x, hidden)\n",
    "        x = self.out(x[:, -1])\n",
    "        return x, hidden\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Sinusoidal Positional Encoding\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_len):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        positions = torch.arange(0, max_len).unsqueeze(1)\n",
    "\n",
    "        frequencies = 10000**(torch.arange(0, d_model, 2)/d_model)\n",
    "        \n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "\n",
    "        self.encoding[:, 1::2] = torch.sin(positions / frequencies)\n",
    "        self.encoding[:, 0::2] = torch.cos(positions / frequencies)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        x = x  + self.encoding[:seq_len].to(x.device)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class CrossLayer(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, dropout):\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.self_attention_encoder_1 = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_size,\n",
    "            nhead=1,\n",
    "            dim_feedforward=hidden_size,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.self_attention_encoder_2 = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_size,\n",
    "            nhead=1,\n",
    "            dim_feedforward=hidden_size,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.cross_attention1 = nn.MultiheadAttention(embed_dim=embed_size, num_heads=1, batch_first=True)\n",
    "        self.cross_attention2 = nn.MultiheadAttention(embed_dim=embed_size, num_heads=1, batch_first=True)\n",
    "\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "\n",
    "        # x1 -> (N, S)\n",
    "        # x2 -> (N, S)\n",
    "\n",
    "        cross_alignment_1, _ = self.cross_attention1(\n",
    "            query=x2,\n",
    "            key=x1,\n",
    "            value=x1\n",
    "        )\n",
    "\n",
    "        cross_alignment_2, _ = self.cross_attention2(\n",
    "            query=x1,\n",
    "            key=x2,\n",
    "            value=x2\n",
    "        )\n",
    "\n",
    "        alignment_1 = self.self_attention_encoder_1(cross_alignment_1)\n",
    "        alignment_2 = self.self_attention_encoder_2(cross_alignment_2)\n",
    "\n",
    "        return alignment_1, alignment_2\n",
    "\n",
    "\n",
    "class CrossNet(nn.Module):\n",
    "    def __init__(self, hidden_size, num_layers, dropout, mode=\"classification\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.is_contrastive = mode==\"contrastive\"\n",
    "\n",
    "        self.drug_embedding = None\n",
    "        self.protein_embedding = None\n",
    "\n",
    "        self.cross_layers = nn.ModuleList([\n",
    "            CrossLayer(embed_size=64, hidden_size=hidden_size, dropout=dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        if not self.is_contrastive:\n",
    "            self.out = nn.Linear(64*2, 2)\n",
    "\n",
    "\n",
    "        self.positional_encoding = PositionalEncoding(d_model=64, max_len=20000)\n",
    "\n",
    "\n",
    "    def set_drug_protein_embeddings(self, drug, protein):\n",
    "        self.drug_embedding = drug\n",
    "        self.protein_embedding = protein\n",
    "\n",
    "        self.drug_embedding.requires_grad = False\n",
    "        self.protein_embedding.requires_grad = False\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1, x2 = x\n",
    "        N = x1.shape[0]\n",
    "\n",
    "        x1 = self.drug_embedding(x1) # (N, S, E)\n",
    "        x2 = self.protein_embedding(x2) # (N, S, E)\n",
    "\n",
    "        x1 = self.positional_encoding(x1)\n",
    "        x2 = self.positional_encoding(x2)\n",
    "\n",
    "        for layer in self.cross_layers:\n",
    "            x1, x2 = layer(x1, x2)\n",
    "\n",
    "\n",
    "        # x1 (N, S, 64)\n",
    "        # x2 (N, S, 64)\n",
    "\n",
    "        x1 = x1.mean(1) # (N, 64)\n",
    "        x2 = x2.mean(1) # (N, 64)\n",
    "\n",
    "        if self.is_contrastive: \n",
    "            return x1, x2\n",
    "        \n",
    "        else:\n",
    "            x = torch.cat([x1, x2], 1) # (N, 64*2)\n",
    "            x = self.out(x)\n",
    "\n",
    "            return x\n",
    "        \n",
    "def build_model(name, config):\n",
    "    hidden_size = config[\"HIDDEN_SIZE\"]\n",
    "    num_layers = config[\"NUM_LAYERS\"]\n",
    "    dropout = config[\"DROPOUT\"]\n",
    "    mode = \"contrastive\" if bool(config[\"CONTRASTIVE\"]) else \"classification\"\n",
    "\n",
    "    \n",
    "    if \"cross\" in name:\n",
    "        net = CrossNet(hidden_size=hidden_size, num_layers=num_layers, dropout=dropout, mode=mode)\n",
    "\n",
    "    else:\n",
    "        net = Transformer(hidden_size=hidden_size, num_layers=num_layers, dropout=dropout)\n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbd67f1",
   "metadata": {},
   "source": [
    "# Vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9056618c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, tokens): \n",
    "        special_tokens = [\"PAD\", \"SOS\", \"EOS\"]\n",
    "        self.tokens = tokens + special_tokens\n",
    "        self.token_ix = {t:i for i, t in enumerate(self.tokens)}\n",
    "        self.ix_token = {i:t for i,t in enumerate(self.tokens)}\n",
    "\n",
    "    def encode(self, seq, max_len=None):\n",
    "        encoded = [self.token_ix[\"SOS\"]] + [self.token_ix[t] for t in seq] + [self.token_ix[\"EOS\"]]\n",
    "        if max_len:\n",
    "            if len(encoded) < max_len:\n",
    "                encoded += [self.token_ix[\"PAD\"]]*(max_len-len(encoded))\n",
    "                \n",
    "        return encoded\n",
    "                \n",
    "    def decode(self, seq):\n",
    "        return [self.ix_token[t] for t in seq]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "    \n",
    "\n",
    "class ProteinVocab(Vocab):\n",
    "    def __init__(self):\n",
    "        # 20 amino acids\n",
    "        tokens = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'Y']\n",
    "        super().__init__(tokens)\n",
    "\n",
    "\n",
    "class SMILEVocab(Vocab):\n",
    "    def __init__(self):\n",
    "        tokens = ['#','(',')','+','-','.','/','1','2','3','4','5','6','7','8','=','@',\n",
    "                  'A','B','C','F','G','H','I','K','L','M','N','O','P','S','T','V','W','Z',\n",
    "                  '[','\\\\',']','a','b','d','e','g','i','l','n','o','r','s','t','u']\n",
    "        super().__init__(tokens)\n",
    "\n",
    "\n",
    "class DrugBankDB:\n",
    "    def __init__(self):\n",
    "        self.id_to_smile = json.load(open(\"../data/databankid_to_smile.json\", \"r\"))\n",
    "\n",
    "    def get_smile_from_id(self, id):\n",
    "        return self.id_to_smile[id]\n",
    "\n",
    "\n",
    "class UniProtDB:\n",
    "    def __init__(self):\n",
    "        self.id_to_amino_seq = json.load(open(\"../data/uniprotid_to_seq.json\", \"r\"))\n",
    "\n",
    "    def get_amino_seq_from_id(self, id):\n",
    "        return self.id_to_amino_seq[id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2c6041",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df509524",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vocab import DrugBankDB, UniProtDB, ProteinVocab, SMILEVocab\n",
    "\n",
    "\n",
    "class DTIDataset(Dataset):\n",
    "    def __init__(self, train, smile_vocab, protein_vocab, smile_embedding, amino_embedding, device=\"cuda\"):\n",
    "        self.device = device\n",
    "        if train:\n",
    "            self.x = json.load(open(\"../data/dti_train_x.json\", \"r\"))\n",
    "            self.y = json.load(open(\"../data/dti_train_y.json\", \"r\"))\n",
    "        else:\n",
    "            self.x = json.load(open(\"../data/dti_test_x.json\", \"r\"))\n",
    "            self.y = json.load(open(\"../data/dti_test_y.json\", \"r\"))\n",
    "\n",
    "        self.train = train\n",
    "        self.drugbankdb = DrugBankDB()\n",
    "        self.uniprotdb = UniProtDB()\n",
    "        self.smile_vocab = smile_vocab\n",
    "        self.protein_vocab = protein_vocab\n",
    "        self.smile_embedding = smile_embedding\n",
    "        self.amino_embedding = amino_embedding\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.x[idx]\n",
    "        \n",
    "        drug = row[\"drug\"]\n",
    "        target = row[\"target\"]\n",
    "        \n",
    "        drug = self.drugbankdb.get_smile_from_id(drug)\n",
    "        drug = torch.Tensor(self.smile_vocab.encode(drug)).long().to(self.device) # get tokenized\n",
    "        # drug = self.smile_embedding(drug).mean(1).squeeze() # get embeddings\n",
    "        \n",
    "        target = self.uniprotdb.get_amino_seq_from_id(target)\n",
    "        target = torch.Tensor(self.protein_vocab.encode(target)).long().to(self.device) # get tokenized\n",
    "        # target = self.amino_embedding(target).mean(1).squeeze() # get embeddings\n",
    "        \n",
    "        y = self.y[idx]\n",
    "\n",
    "        drug = drug.to(self.device)\n",
    "        target = target.to(self.device)\n",
    "        \n",
    "        return drug, target, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99ef5e7",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb31cb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_yaml(f):\n",
    "    return yaml.safe_load(open(f, \"r\"))\n",
    "\n",
    "def get_predictions_from_similarity(repr1, repr2, thresh=.5):\n",
    "    repr1 = normalize(repr1, p=2, dim=1) # l2 norm\n",
    "    repr2 = normalize(repr2, p=2, dim=1) # l2 norm \n",
    "    distance = torch.abs(cosine_similarity(repr1, repr2)) # (N)\n",
    "    \n",
    "    pred = (distance > thresh).long()\n",
    "\n",
    "    return pred\n",
    "\n",
    "def get_predictions_from_prob(prob, thresh=.5):\n",
    "    return (prob > thresh).long()\n",
    "\n",
    "\n",
    "def dataloader_collate_fn(data):\n",
    "    \"\"\"\n",
    "    Pad (x1, x2) in data to same length in batch\n",
    "    \"\"\"\n",
    "\n",
    "    x1, x2, y = zip(*data)\n",
    "\n",
    "    y = torch.Tensor(list(y))\n",
    "\n",
    "    lengths_x1 = [len(x) for x in x1]\n",
    "    lengths_x2 = [len(x) for x in x2]\n",
    "\n",
    "    max_len_x1 = max(lengths_x1)\n",
    "    max_len_x2 = max(lengths_x2)\n",
    "\n",
    "    batch_size = len(lengths_x1)\n",
    "\n",
    "    x1_padded = torch.zeros(batch_size, max_len_x1).fill_(51)\n",
    "    x2_padded = torch.zeros(batch_size, max_len_x2).fill_(21) # 21 is the <PAD> token in the amino acids vocab\n",
    "\n",
    "    for i, seq in enumerate(x1):\n",
    "        end = lengths_x1[i]\n",
    "        x1_padded[i, :end] = seq[:end]\n",
    "\n",
    "    for i, seq in enumerate(x2):\n",
    "        end = lengths_x2[i]\n",
    "        x2_padded[i, :end] = seq[:end]\n",
    "\n",
    "\n",
    "    x1_padded = x1_padded.long()\n",
    "    x2_padded = x2_padded.long()\n",
    "\n",
    "\n",
    "    return x1_padded, x2_padded, y\n",
    "\n",
    "def accuracy_from_contrastive_model(p, y):\n",
    "    p1, p2 = p\n",
    "    p = get_predictions_from_similarity(p1, p2)\n",
    "    accuracy = (p == y).float().mean()\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def accuracy_from_classification_model(p, y):\n",
    "    p = p.argmax(-1)\n",
    "    accuracy = (p==y).float().mean()\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f8c8d7",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b83fb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "class RunningAverager:\n",
    "    def __init__(self, track=[], smooth=.6):\n",
    "        self.track = {\n",
    "            to_track: 0\n",
    "            for to_track in track\n",
    "        }\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def add_new(self, values):\n",
    "        for key in values.keys():\n",
    "            if key not in self.track:\n",
    "                logging.warning(f\"{key} not in tracked values.\")\n",
    "            \n",
    "            new_value = values[key]\n",
    "            old_value = self.track[key]\n",
    "\n",
    "            self.track[key] = self.smooth*new_value + (1-self.smooth)*old_value\n",
    "\n",
    "    def get_tracked(self):\n",
    "        return self.track\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        train,\n",
    "        test,\n",
    "        epochs,\n",
    "        optimizer,\n",
    "        lossfn,\n",
    "        metrics,\n",
    "        config_file,\n",
    "        smooth=.6,\n",
    "        device=\"cuda\"\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.model.to(device)\n",
    "        self.optimizer = optimizer\n",
    "        self.lossfn = lossfn\n",
    "        self.metrics = metrics\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        self.epochs = epochs\n",
    "        self.writer = writer\n",
    "        self.smooth = smooth\n",
    "        self.device = device\n",
    "        self.config_file = config_file\n",
    "\n",
    "        self.lowest_loss = {\n",
    "            \"train\": float(\"inf\"),\n",
    "            \"test\": float(\"inf\")\n",
    "        }\n",
    "\n",
    "    def train_one_epoch(self, epoch):\n",
    "        averager = RunningAverager(\n",
    "            track=[\"loss\"]+list(self.metrics.keys()),\n",
    "            smooth=self.smooth\n",
    "        )\n",
    "\n",
    "        for x1, x2, y in self.train:\n",
    "            self.model.train()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            x1 = x1.to(self.device)\n",
    "            x2 = x2.to(self.device)\n",
    "            y = y.to(self.device)\n",
    "\n",
    "            p = self.model((x1, x2))\n",
    "            loss = self.lossfn(p, y)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            self.optimizer.step()\n",
    "\n",
    "            averager.add_new({\n",
    "                \"loss\": loss.item(),\n",
    "            })\n",
    "\n",
    "            for metric_name in self.metrics.keys():\n",
    "                metric_value = self.metrics[metric_name](p, y)\n",
    "                \n",
    "                averager.add_new({\n",
    "                    metric_name: metric_value\n",
    "                })\n",
    "\n",
    "        averaged = averager.get_tracked()\n",
    "\n",
    "        return averaged\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def test_one_epoch(self, epoch):\n",
    "        self.model.eval()\n",
    "        averager = RunningAverager(\n",
    "            track=[\"loss\"]+list(self.metrics.keys()),\n",
    "            smooth=self.smooth\n",
    "        )\n",
    "\n",
    "        for x1, x2, y in self.test:\n",
    "\n",
    "            x1 = x1.to(self.device)\n",
    "            x2 = x2.to(self.device)\n",
    "            y = y.to(self.device)\n",
    "            \n",
    "            p = self.model((x1, x2))\n",
    "            loss = self.lossfn(p, y)\n",
    "\n",
    "            averager.add_new({\n",
    "                \"loss\": loss.item(),\n",
    "            })\n",
    "\n",
    "            for metric_name in self.metrics.keys():\n",
    "                metric_value = self.metrics[metric_name](p, y)\n",
    "                \n",
    "                averager.add_new({\n",
    "                    metric_name: metric_value\n",
    "                })\n",
    "\n",
    "        averaged = averager.get_tracked()\n",
    "\n",
    "        return averaged\n",
    "\n",
    "    def run(self):\n",
    "        for epoch in tqdm(range(self.epochs)):\n",
    "            train_averaged = self.train_one_epoch(epoch)\n",
    "            test_averaged = self.test_one_epoch(epoch)\n",
    "\n",
    "            if test_averaged[\"loss\"] < self.lowest_loss[\"test\"]:\n",
    "                self.lowest_loss[\"test\"] = test_averaged[\"loss\"]\n",
    "                \n",
    "                self.save_checkpoint(accuracy=test_averaged[\"Accuracy\"], epoch=epoch)\n",
    "\n",
    "            if train_averaged[\"loss\"] < self.lowest_loss[\"train\"]:\n",
    "                self.lowest_loss[\"train\"] = train_averaged[\"loss\"]\n",
    "                \n",
    "                \n",
    "            print(\n",
    "                \" \".join([f\"{metric_name}: {averaged[metric_name]}\" for metric_name in list(train_averaged.keys())])\n",
    "            )\n",
    "            \n",
    "            print(\n",
    "                \" \".join([f\"{metric_name}: {averaged[metric_name]}\" for metric_name in list(test_averaged.keys())])\n",
    "            )\n",
    "            \n",
    "    def save_checkpoint(self, accuracy=None, epoch=None):\n",
    "        torch.save({\n",
    "            \"model\": self.model.state_dict(),\n",
    "            \"optimizer\": self.optimizer.state_dict(),\n",
    "            \"loss\": self.lowest_loss[\"test\"],\n",
    "            \"accuracy\": accuracy,\n",
    "            \"epoch\": epoch+1\n",
    "        }, f\"../checkpoints/classification/{self.config_file['RUN_NAME']}/model.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3838750b",
   "metadata": {},
   "source": [
    "# Train tyme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2832e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"SEED\": 42,\n",
    "    \"NAME\": \"crosstransformer\",\n",
    "    \"RUN_NAME\": \"crosstransformer-contrastive-baseline\",\n",
    "    \"LR\": 1e-4,\n",
    "    \"EPOCHS\": 3000,\n",
    "    \"BATCH_SIZE\": 16,\n",
    "    \"DROPOUT\": 0,\n",
    "    \"NUM_LAYERS\": 2,\n",
    "    \"HIDDEN_SIZE\": 256,\n",
    "    \"CONTRASTIVE\": True\n",
    "    \n",
    "}\n",
    "\n",
    "os.makedirs(f\"../checkpoints/classification/{config['RUN_NAME']}\")\n",
    "\n",
    "np.random.seed(int(config[\"SEED\"]))\n",
    "torch.manual_seed(int(config[\"SEED\"]))\n",
    "\n",
    "amino_vocab = ProteinVocab()\n",
    "smile_vocab = SMILEVocab()\n",
    "\n",
    "pretrained_smile_embeddings = GRUModel(\n",
    "    vocab_size=len(smile_vocab),\n",
    "    embed_size=64,\n",
    "    hidden_size=128,\n",
    "    num_layers=1,\n",
    "    dropout=0,\n",
    "    bidirectional=False\n",
    ").to(args.device)\n",
    "\n",
    "pretrained_amino_embeddings = GRUModel(\n",
    "    vocab_size=len(amino_vocab),\n",
    "    embed_size=64,\n",
    "    hidden_size=128,\n",
    "    num_layers=1,\n",
    "    dropout=0,\n",
    "    bidirectional=False\n",
    ").to(args.device)\n",
    "\n",
    "pretrained_smile_embeddings.load_state_dict(torch.load(\"../checkpoints/pretraining/smile_gru.pth\"))\n",
    "pretrained_amino_embeddings.load_state_dict(torch.load(\"../checkpoints/pretraining/protein_gru.pth\"))\n",
    "\n",
    "pretrained_smile_embeddings = pretrained_smile_embeddings.embedding\n",
    "pretrained_amino_embeddings = pretrained_amino_embeddings.embedding\n",
    "\n",
    "\n",
    "net = build_model(config)\n",
    "\n",
    "net.set_drug_protein_embeddings(\n",
    "    pretrained_smile_embeddings.to(args.device),\n",
    "    pretrained_amino_embeddings.to(args.device)        \n",
    ")\n",
    "\n",
    "net.to(args.device)\n",
    "\n",
    "train = DTIDataset(\n",
    "    train=True,\n",
    "    smile_vocab=smile_vocab,\n",
    "    protein_vocab=amino_vocab,\n",
    "    smile_embedding=pretrained_smile_embeddings,\n",
    "    amino_embedding=pretrained_amino_embeddings,\n",
    "    device=args.device\n",
    ")\n",
    "\n",
    "test = DTIDataset(\n",
    "    train=False,\n",
    "    smile_vocab=smile_vocab,\n",
    "    protein_vocab=amino_vocab,\n",
    "    smile_embedding=pretrained_smile_embeddings,\n",
    "    amino_embedding=pretrained_amino_embeddings,\n",
    "    device=args.device\n",
    ")\n",
    "\n",
    "train = DataLoader(train, batch_size=int(config_file[\"BATCH_SIZE\"]), shuffle=True, collate_fn=utils.dataloader_collate_fn)\n",
    "test = DataLoader(test, batch_size=int(config_file[\"BATCH_SIZE\"]), shuffle=True, collate_fn=utils.dataloader_collate_fn)\n",
    "\n",
    "lossfn = ContrastiveLoss()\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=float(config_file[\"LR\"]))\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=net,\n",
    "    train=train,\n",
    "    test=test,\n",
    "    epochs=config_file[\"EPOCHS\"],\n",
    "    optimizer=optimizer,\n",
    "    lossfn=lossfn,\n",
    "    metrics={\n",
    "        \"Accuracy\": accuracy_from_contrastive_model\n",
    "    },\n",
    "    smooth=.6,\n",
    "    device=\"cuda\",\n",
    "    config_file=config\n",
    ")\n",
    "\n",
    "trainer.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
